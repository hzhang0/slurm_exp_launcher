# Slurm Exp Laucher

A lightweight system for launching grids of Python jobs in a Slurm cluster.

## Features

- Wraps around any Python training script that uses argparse for arguments.
- Define hyperparameter grids in an intuitive way using Python dictionaries.
- Launcher knows which jobs are running, and will only submit jobs that have crashed or have not been started.
- Compatible with most experiment tracking systems (e.g. wandb).
- Also compatible with running on a local server using multiple GPUs simultaneously.


## Acknowledgements

This launcher is a heavily modified version of the experiment launcher in the [DomainBed](https://github.com/facebookresearch/DomainBed) repository.

## Usage

### 1. Setting Up

- Copy `experiments.py`, `launchers.py`, and `sweep.py` into your project repository.
- Go through `train.py`, and add the required components to your own training script.
- In L33 of `sweep.py`, modify the launch command to match your repository structure.

### 2. Writing an Experiment in `experiments.py`
 Generally, an experiment is a class with a `get_hparams()` function which returns a list of dictionaries, each specifying a job, with keys being arguments present in your training script. Simple examples of experiments are found in `experiments.py`. 
 
 Examples of more complex experiments from previously published works can be found [here](https://github.com/MLforHealth/expl_perf_drop/blob/main/expl_perf_drop/experiments.py) and [here](https://github.com/MLforHealth/predictive_checklists/blob/master/scripts/experiments.py).

### 3. Launching a Sweep


Call `sweep.py` with the experiment name (the name of a class in `experiments.py`), a launcher name (in the registry in `launchers.py`), and other appropriate arguments to launch an experiment (a grid of jobs). For example:

```
slurm_pre="--mem 50gb -c 8 --gres gpu:1 -t 7-00:00 -p healthyml -q healthyml-main --output {SLURM_LOG_DIR}/%A.log" # sbatch arguments
python sweep.py launch \
   --experiment SampleExp1 \
   --slurm_pre "${slurm_pre}" \
   --command_launcher "slurm" \
   --output_root "YOUR_OUTPUT_DIR/SampleExp1" \
   --max_slurm_jobs 20
```

Before running this, make sure to activate your conda environment. You may want to put this into a bash script for convenience. Note that to avoid folder naming conflicts, each job is stored in a unique folder generated by hashing its args.

### 4. Aggregating Results

After some (or all) jobs have finished running, you can load the results that have been saved by your training script, with something like this:
```
import pandas as pd
from pathlib import Path
import pickle
import json

output_root = Path('YOUR_OUTPUT_DIR')
ress = []

for i in output_root.glob('**/done'):
    args = json.load((i.parent/'args.json').open('r'))
    res = pickle.load((i.parent/'results.pkl').open('rb'))
    ress.append({**args, **res})

df = pd.DataFrame(ress)
```
